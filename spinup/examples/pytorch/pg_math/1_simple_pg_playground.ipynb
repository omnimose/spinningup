{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make environment, check spaces, get obs / act dims\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()  \n",
    "env.render()\n",
    "\n",
    "assert isinstance(env.observation_space, Box), \\\n",
    "    \"This example only works for envs with continuous state spaces.\"\n",
    "assert isinstance(env.action_space, Discrete), \\\n",
    "    \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_acts = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 2\n",
      "Log probability of action 1: -1.2039728164672852\n",
      "Log probabilities of all actions: tensor([-2.3026, -1.2040, -0.5108])\n",
      "Entropy: 0.897945761680603\n"
     ]
    }
   ],
   "source": [
    "# torch.distributions.Categorical usage\n",
    "\n",
    "# Define a probability distribution\n",
    "probs = torch.tensor([0.1, 0.3, 0.6])  # Must sum to 1\n",
    "dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "# Sample from the distribution\n",
    "sample = dist.sample()\n",
    "print(\"Sampled action:\", sample.item())\n",
    "\n",
    "# Get log probability of a specific action\n",
    "log_prob = dist.log_prob(torch.tensor(1))\n",
    "\n",
    "# math.log(0.3) = -1.2039728043259361\n",
    "print(\"Log probability of action 1:\", log_prob.item())\n",
    "\n",
    "indices = torch.arange(len(probs))  # Tensor of indices [0, 1, 2]\n",
    "log_probs = dist.log_prob(indices)\n",
    "print(\"Log probabilities of all actions:\", log_probs)\n",
    "\n",
    "# Entropy of the distribution\n",
    "entropy = dist.entropy()\n",
    "print(\"Entropy:\", entropy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2039728043259361"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8979457248567797\n"
     ]
    }
   ],
   "source": [
    "p = [0.1, 0.3, 0.6]\n",
    "\n",
    "# Calculate the entropy term: p_i * log(p_i)\n",
    "entropy_term = sum(p_i * math.log(p_i) for p_i in p)\n",
    "\n",
    "entropy_term = -entropy_term  # Negate the sum to get the entropy\n",
    "\n",
    "print(entropy_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return torch.distributions.Categorical(logits=logits)  # Convert logits to a categorical distribution\n",
    "\n",
    "# Example usage\n",
    "policy_net = PolicyNetwork(4, 2)  # Example: 4 input features, 2 possible actions\n",
    "state = torch.rand(4)  # Example input\n",
    "dist = policy_net(state)\n",
    "action = dist.sample()\n",
    "log_prob = dist.log_prob(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2992, -0.1901, -0.2784, -0.1818],\n",
      "        [ 0.2992,  0.1901,  0.2784,  0.1818]])\n",
      "tensor([-0.3202,  0.3202])\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return torch.distributions.Categorical(logits=logits)  # Categorical distribution from logits\n",
    "\n",
    "# Example usage\n",
    "policy_net = PolicyNetwork(4, 2)  # 4 input features, 2 possible actions\n",
    "state = torch.rand(4, requires_grad=True)  # Example state input (with gradient tracking)\n",
    "\n",
    "# Forward pass to get the distribution\n",
    "dist = policy_net(state)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability of the action\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "# Define the loss as -log_prob * 2\n",
    "loss = -log_prob * 2\n",
    "\n",
    "# Backpropagate to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Get the gradients of the Policy Network parameters\n",
    "for param in policy_net.parameters():\n",
    "    print(param.grad)  # This will print the gradients of each parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient: tensor([-0.3202,  0.3202], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "# Example usage\n",
    "policy_net = PolicyNetwork(4, 2)  # 4 input features, 2 possible actions\n",
    "\n",
    "# Seed the state to ensure reproducibility\n",
    "state = torch.rand(4, requires_grad=True)  # Example state input (with gradient tracking)\n",
    "\n",
    "# Forward pass to get the distribution\n",
    "dist = policy_net(state)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability of the action\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "# Define the loss as -log_prob * 2\n",
    "loss = -log_prob * 2\n",
    "\n",
    "# Manually calculate the gradient of the loss with respect to the logits\n",
    "logits = dist.logits  # The logits produced by the network\n",
    "probs = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "\n",
    "# Manually calculate the gradient of the loss w.r.t logits\n",
    "log_prob = dist.log_prob(action)\n",
    "chosen_action_prob = probs[action]  # Probability of the chosen action\n",
    "\n",
    "# Gradient of the loss with respect to the logit of the chosen action\n",
    "grad_manual = -2 * (torch.eye(len(probs))[action] - probs)\n",
    "\n",
    "print(\"Manual Gradient:\", grad_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually computed gradient with respect to logits:\n",
      "tensor([-0.3202,  0.3202], grad_fn=<CopySlices>)\n",
      "Manually computed gradient with respect to fc weights:\n",
      "tensor([[-0.2992, -0.1901, -0.2784, -0.1818],\n",
      "        [ 0.2992,  0.1901,  0.2784,  0.1818]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Automatic gradients computed by PyTorch:\n",
      "tensor([[-0.2992, -0.1901, -0.2784, -0.1818],\n",
      "        [ 0.2992,  0.1901,  0.2784,  0.1818]])\n",
      "tensor([-0.3202,  0.3202])\n",
      "\n",
      "Gradient with respect to fc weights (PyTorch):\n",
      "tensor([[-0.2992, -0.1901, -0.2784, -0.1818],\n",
      "        [ 0.2992,  0.1901,  0.2784,  0.1818]])\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "# Example usage\n",
    "policy_net = PolicyNetwork(4, 2)  # 4 input features, 2 possible actions\n",
    "\n",
    "# Seed the state to ensure reproducibility\n",
    "state = torch.rand(4, requires_grad=True)  # Example state input (with gradient tracking)\n",
    "\n",
    "# Forward pass to get the distribution\n",
    "dist = policy_net(state)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability of the action\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "# Define the loss as -log_prob * 2\n",
    "loss = -log_prob * 2\n",
    "\n",
    "# Manually calculate the gradient of the loss with respect to the logits\n",
    "logits = dist.logits  # The logits produced by the network\n",
    "probs = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "\n",
    "# Get the chosen action's probability\n",
    "chosen_action_prob = probs[action]  # Probability of the chosen action\n",
    "\n",
    "# Gradient of the loss with respect to logits (manual)\n",
    "grad_logit = torch.zeros_like(logits)\n",
    "\n",
    "# For the chosen action (a)\n",
    "grad_logit[action] = -2 * (1 - chosen_action_prob)\n",
    "\n",
    "# For the other actions (i != a)\n",
    "for i in range(len(probs)):\n",
    "    if i != action:\n",
    "        grad_logit[i] = 2 * probs[i]\n",
    "\n",
    "# Print the manually computed gradient\n",
    "print(\"Manually computed gradient with respect to logits:\")\n",
    "print(grad_logit)\n",
    "\n",
    "# Manually compute the gradient with respect to the weights of the fc layer\n",
    "# The input to the fc layer is the state, so we multiply by the gradients\n",
    "grad_fc_weight = grad_logit.view(-1, 1) * state  # Gradient with respect to weights\n",
    "\n",
    "# Print the manually computed gradient with respect to the fc weights\n",
    "print(\"Manually computed gradient with respect to fc weights:\")\n",
    "print(grad_fc_weight)\n",
    "\n",
    "# Backpropagate to compute gradients automatically using PyTorch\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nAutomatic gradients computed by PyTorch:\")\n",
    "\n",
    "# Get the gradients of the Policy Network parameters\n",
    "for param in policy_net.parameters():\n",
    "    print(param.grad)  # This will print the gradients of each parameter\n",
    "\n",
    "# Print the gradient with respect to the weights of the fully connected layer\n",
    "print(\"\\nGradient with respect to fc weights (PyTorch):\")\n",
    "print(policy_net.fc.weight.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
