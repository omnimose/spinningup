{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make environment, check spaces, get obs / act dims\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()  \n",
    "env.render()\n",
    "\n",
    "assert isinstance(env.observation_space, Box), \\\n",
    "    \"This example only works for envs with continuous state spaces.\"\n",
    "assert isinstance(env.action_space, Discrete), \\\n",
    "    \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_acts = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 2\n",
      "Log probability of action 1: -1.2039728164672852\n",
      "Log probabilities of all actions: tensor([-2.3026, -1.2040, -0.5108])\n",
      "Entropy: 0.897945761680603\n"
     ]
    }
   ],
   "source": [
    "# torch.distributions.Categorical usage\n",
    "\n",
    "# Define a probability distribution\n",
    "probs = torch.tensor([0.1, 0.3, 0.6])  # Must sum to 1\n",
    "dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "# Sample from the distribution\n",
    "sample = dist.sample()\n",
    "print(\"Sampled action:\", sample.item())\n",
    "\n",
    "# Get log probability of a specific action\n",
    "log_prob = dist.log_prob(torch.tensor(1))\n",
    "\n",
    "# math.log(0.3) = -1.2039728043259361\n",
    "print(\"Log probability of action 1:\", log_prob.item())\n",
    "\n",
    "indices = torch.arange(len(probs))  # Tensor of indices [0, 1, 2]\n",
    "log_probs = dist.log_prob(indices)\n",
    "print(\"Log probabilities of all actions:\", log_probs)\n",
    "\n",
    "# Entropy of the distribution\n",
    "entropy = dist.entropy()\n",
    "print(\"Entropy:\", entropy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2039728043259361"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8979457248567797\n"
     ]
    }
   ],
   "source": [
    "p = [0.1, 0.3, 0.6]\n",
    "\n",
    "# Calculate the entropy term: p_i * log(p_i)\n",
    "entropy_term = sum(p_i * math.log(p_i) for p_i in p)\n",
    "\n",
    "entropy_term = -entropy_term  # Negate the sum to get the entropy\n",
    "\n",
    "print(entropy_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return torch.distributions.Categorical(logits=logits)  # Convert logits to a categorical distribution\n",
    "\n",
    "# Example usage\n",
    "policy_net = PolicyNetwork(4, 2)  # Example: 4 input features, 2 possible actions\n",
    "state1 = torch.rand(4)  # Example input\n",
    "dist = policy_net(state1)\n",
    "action = dist.sample()\n",
    "log_prob = dist.log_prob(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2992, -0.1901, -0.2784, -0.1818],\n",
      "        [ 0.2992,  0.1901,  0.2784,  0.1818]])\n",
      "tensor([-0.3202,  0.3202])\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return torch.distributions.Categorical(logits=logits)  # Categorical distribution from logits\n",
    "\n",
    "# Example usage\n",
    "policy_net = PolicyNetwork(4, 2)  # 4 input features, 2 possible actions\n",
    "state1 = torch.rand(4, requires_grad=True)  # Example state input (with gradient tracking)\n",
    "\n",
    "# Forward pass to get the distribution\n",
    "dist = policy_net(state1)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability of the action\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "# Define the loss as -log_prob * 2\n",
    "loss = -log_prob * 2\n",
    "\n",
    "# Backpropagate to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Get the gradients of the Policy Network parameters\n",
    "for param in policy_net.parameters():\n",
    "    print(param.grad)  # This will print the gradients of each parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient: tensor([-0.3202,  0.3202], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "# Example usage\n",
    "policy_net = PolicyNetwork(4, 2)  # 4 input features, 2 possible actions\n",
    "\n",
    "# Seed the state to ensure reproducibility\n",
    "state1 = torch.rand(4, requires_grad=True)  # Example state input (with gradient tracking)\n",
    "\n",
    "# Forward pass to get the distribution\n",
    "dist = policy_net(state1)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability of the action\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "# Define the loss as -log_prob * 2\n",
    "loss = -log_prob * 2\n",
    "\n",
    "# Manually calculate the gradient of the loss with respect to the logits\n",
    "logits = dist.logits  # The logits produced by the network\n",
    "probs = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "\n",
    "# Manually calculate the gradient of the loss w.r.t logits\n",
    "log_prob = dist.log_prob(action)\n",
    "chosen_action_prob = probs[action]  # Probability of the chosen action\n",
    "\n",
    "# Gradient of the loss with respect to the logit of the chosen action\n",
    "grad_manual = -2 * (torch.eye(len(probs))[action] - probs)\n",
    "\n",
    "print(\"Manual Gradient:\", grad_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually computed gradient with respect to logits:\n",
      "tensor([ 1.3456, -1.3456], grad_fn=<CopySlices>)\n",
      "Manually computed gradient with respect to fc weights:\n",
      "tensor([[ 1.2659,  0.1792,  1.2576,  0.7987],\n",
      "        [-1.2659, -0.1792, -1.2576, -0.7987]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Automatic gradients computed by PyTorch:\n",
      "fc.weight: tensor([[ 1.2659,  0.1792,  1.2576,  0.7987],\n",
      "        [-1.2659, -0.1792, -1.2576, -0.7987]])\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, bias=False)  # No bias for simplicity\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "# Example usage\n",
    "policy_net = PolicyNetwork(4, 2)  # 4 input features, 2 possible actions\n",
    "\n",
    "# Seed the state to ensure reproducibility\n",
    "state1 = torch.rand(4, requires_grad=True)  # Example state input (with gradient tracking)\n",
    "\n",
    "# Forward pass to get the distribution\n",
    "dist = policy_net(state1)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability of the action\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "# Define the loss as -log_prob * 2\n",
    "loss = -log_prob * 2\n",
    "\n",
    "# Manually calculate the gradient of the loss with respect to the logits\n",
    "logits = dist.logits  # The logits produced by the network\n",
    "probs = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "\n",
    "# Get the chosen action's probability\n",
    "chosen_action_prob = probs[action]  # Probability of the chosen action\n",
    "\n",
    "# Gradient of the loss with respect to logits (manual)\n",
    "grad_logit = torch.zeros_like(logits)\n",
    "\n",
    "# For the chosen action (a)\n",
    "grad_logit[action] = -2 * (1 - chosen_action_prob)\n",
    "\n",
    "# For the other actions (i != a)\n",
    "for i in range(len(probs)):\n",
    "    if i != action:\n",
    "        grad_logit[i] = 2 * probs[i]\n",
    "\n",
    "# Print the manually computed gradient\n",
    "print(\"Manually computed gradient with respect to logits:\")\n",
    "print(grad_logit)\n",
    "\n",
    "# Manually compute the gradient with respect to the weights of the fc layer\n",
    "# The input to the fc layer is the state, so we multiply by the gradients\n",
    "grad_fc_weight = grad_logit.view(-1, 1) * state1  # Gradient with respect to weights\n",
    "\n",
    "# Print the manually computed gradient with respect to the fc weights\n",
    "print(\"Manually computed gradient with respect to fc weights:\")\n",
    "print(grad_fc_weight)\n",
    "\n",
    "# Backpropagate to compute gradients automatically using PyTorch\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nAutomatic gradients computed by PyTorch:\")\n",
    "\n",
    "# Get the gradients of the Policy Network parameters\n",
    "for name, param in policy_net.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: {param.grad}\")\n",
    "\n",
    "# Print the gradient with respect to the weights of the fully connected layer\n",
    "# print(\"\\nGradient with respect to fc weights (PyTorch):\")\n",
    "# print(policy_net.fc.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Gradients w.r.t. Parameters ----\n",
      "fc.weight: tensor([[ 2.0169,  1.3109,  1.0185,  1.2691],\n",
      "        [ 0.9435,  0.6132,  0.4744,  0.5945],\n",
      "        [-2.9604, -1.9241, -1.4928, -1.8635]])\n",
      "\n",
      "---- Gradients w.r.t. Logits ----\n",
      "Manually Computed Logits Gradient:\n",
      " tensor([[ 2.0169,  1.3109,  1.0185,  1.2691],\n",
      "        [ 0.9435,  0.6132,  0.4744,  0.5945],\n",
      "        [-2.9604, -1.9241, -1.4928, -1.8635]], grad_fn=<AddBackward0>)\n",
      "\n",
      "✅ Manual gradients match PyTorch's backward()!\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define a simple policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, bias=False)  # Fully connected layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)  # Raw scores (logits)\n",
    "        return torch.distributions.Categorical(logits=logits)  # Categorical distribution\n",
    "\n",
    "# Initialize network\n",
    "input_dim = 4\n",
    "output_dim = 3  # Assume 3 possible actions\n",
    "policy_net = PolicyNetwork(input_dim, output_dim)\n",
    "\n",
    "# Seed the state to ensure reproducibility\n",
    "state1 = torch.rand(input_dim)  # Example input state\n",
    "state2 = torch.rand(input_dim)  # Another input state\n",
    "\n",
    "# Forward pass: Get action distributions\n",
    "dist1 = policy_net(state1)\n",
    "dist2 = policy_net(state2)\n",
    "\n",
    "# Sample actions from the distributions\n",
    "action1 = dist1.sample()\n",
    "action2 = dist2.sample()\n",
    "\n",
    "# Compute log probabilities of the actions\n",
    "log_prob1 = dist1.log_prob(action1)\n",
    "log_prob2 = dist2.log_prob(action2)\n",
    "\n",
    "# Define the losses\n",
    "loss1 = -log_prob1 * 2\n",
    "loss2 = -log_prob2 * 3\n",
    "total_loss = (loss1 + loss2).mean()\n",
    "\n",
    "# Compute gradients using PyTorch\n",
    "policy_net.zero_grad()\n",
    "total_loss.backward()\n",
    "\n",
    "# Print the gradients of the parameters\n",
    "print(\"\\n---- Gradients w.r.t. Parameters ----\")\n",
    "for name, param in policy_net.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: {param.grad}\")\n",
    "        \n",
    "\n",
    "# Extract PyTorch computed gradients\n",
    "grad_fc_weight = policy_net.fc.weight.grad\n",
    "\n",
    "# ------------------------------------------\n",
    "# Manual Gradient Computation\n",
    "# ------------------------------------------\n",
    "\n",
    "# Compute probabilities from softmax\n",
    "logits1 = policy_net.fc(state1)  # Logits for state1\n",
    "logits2 = policy_net.fc(state2)  # Logits for state2\n",
    "\n",
    "probs1 = torch.softmax(logits1, dim=-1)  # Probabilities for state1\n",
    "probs2 = torch.softmax(logits2, dim=-1)  # Probabilities for state2\n",
    "\n",
    "# Compute the manual gradient for logits using log-softmax trick\n",
    "grad_logits1 = -2 * (torch.eye(output_dim)[action1] - probs1)  # Gradient of log_prob1 w.r.t logits1\n",
    "grad_logits2 = -3 * (torch.eye(output_dim)[action2] - probs2)  # Gradient of log_prob2 w.r.t logits2\n",
    "\n",
    "# Compute manual gradients for network weights\n",
    "grad_fc_weight_manual = torch.outer(grad_logits1, state1) + torch.outer(grad_logits2, state2)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Compare Manual Gradients to PyTorch\n",
    "# ------------------------------------------\n",
    "print(\"\\n---- Gradients w.r.t. Logits ----\")\n",
    "print(\"Manually Computed Logits Gradient:\\n\", grad_fc_weight_manual)\n",
    "\n",
    "\n",
    "# Check if the gradients match\n",
    "assert torch.allclose(grad_fc_weight, grad_fc_weight_manual, atol=1e-1), \"Mismatch in weight gradients!\"\n",
    "\n",
    "print(\"\\n✅ Manual gradients match PyTorch's backward()!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
